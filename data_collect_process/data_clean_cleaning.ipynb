{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import langdetect \n",
    "import re\n",
    "\n",
    "def data_clean(typee, label):\n",
    "    def string_clean(s):\n",
    "        string = re.sub(r'<.*?>','', s) # remove html tag\n",
    "        string = re.sub(r'http[s]?://\\S+', '', string) # remove url\n",
    "        string = re.sub(r'\\\\n', ' ', string) # remove new line\n",
    "        string = re.sub(r'[^a-zA-Z0-9\\s]', ' ', string) # remove sepcial characters\n",
    "        string = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', string) # separate sticky words\n",
    "        string = re.sub(r\"(?<!\\S)[^aA](?!\\S)\", \" \", string)\n",
    "        words = re.split(r'\\s+', string.strip())\n",
    "        new_string = ' '.join(words)\n",
    "        return new_string\n",
    "\n",
    "    file = open(f'./log/data_clean_log/{typee}_repos_with_no_read_me.txt', 'a')\n",
    "    file2 = open(f'./log/data_clean_log/{typee}_repos_not_english.txt','a')\n",
    "    df = pd.read_csv(f'{typee}_repos.csv')\n",
    "    new_df = []\n",
    "    for row in df.iterrows():\n",
    "        idx = row[0]\n",
    "        name = df.iloc[idx]['name']\n",
    "        stars = df.iloc[idx]['stars']\n",
    "        url = df.iloc[idx]['url']\n",
    "        defaultBranch = df.iloc[idx]['defaultBranche']\n",
    "        des = str(df.iloc[idx]['description'])\n",
    "        readme = str(df.iloc[idx]['readme'])\n",
    "\n",
    "        # check if t\n",
    "        if readme == '[]' or readme=='['']':\n",
    "            file.write(f'{name},{des},{stars},{url},{defaultBranch},{readme}\\n')\n",
    "            continue\n",
    "\n",
    "        # check if the repos is English\n",
    "        try:\n",
    "            if langdetect.detect(readme) != 'en':\n",
    "                file2.write(f'{name},{des},{stars},{url},{defaultBranch},{readme}\\n')\n",
    "                continue\n",
    "        except:\n",
    "            print(readme)\n",
    "            continue\n",
    "        # clean special character for README.md and description\n",
    "        readme = string_clean(readme)\n",
    "        des = string_clean(des)\n",
    "\n",
    "        new_df.append((name, des, stars, url, defaultBranch, readme))\n",
    "\n",
    "    new_df = pd.DataFrame(data=new_df, columns=['name','description','stars','url',\n",
    "                                                'defaultBranche','readme'])\n",
    "\n",
    "    new_df['label'] = label\n",
    "    new_df.to_csv(f'clean_{typee}_repos.csv', index=False)\n",
    "    file.close()\n",
    "    file2.close()\n",
    "\n",
    "TYPE = ['database','framework','library', 'pl','platform','plugin','toolkit']\n",
    "for label, t in enumerate(TYPE):\n",
    "    data_clean(t, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Minh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "stopwordsSet = set(stopwords.words(\"english\"))\n",
    "stopwordsSet.add('use')\n",
    "stopwordsSet.add('using')\n",
    "stopwordsSet.add('used')\n",
    "stopwordsSet.add('user')\n",
    "\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    cleanedWord = ' '.join([word for word in words.split() if word.lower()  not in stopwordsSet \n",
    "                            and len(word) > 1])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                    background_color=color,\n",
    "                    width=2500,\n",
    "                    height=2000\n",
    "                    ).generate(cleanedWord)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
